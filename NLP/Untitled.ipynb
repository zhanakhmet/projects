{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "text = 'The next step is to convert pre-processed tokens into a dictionary with word index and it’s count in the corpus. We can use gensim package to create this dictionary then to create bag-of-words.'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_lemma_stopwords(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower()) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if t.isalpha()] # keep strings with only alphabets\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "def dataCleaning(data):\n",
    "    data[\"content\"] = data[\"content\"].apply(tokenize_lemma_stopwords)\n",
    "    return data\n",
    "tokenize_lemma_stopwords(text)\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8616ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
    "\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word), \n",
    "            stemmer_lancaster.stem(word), stemmer_snowball.stem(word)]\n",
    "    print (formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55ffbc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhana\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matrix type must be 'f', 'd', 'F', or 'D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7832/3413636720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mlsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'arpack'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdtm_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mdtm_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_lsa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"arpack\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mv0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_init_arpack_v0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[1;31m# svds doesn't abide by scipy.linalg.svd/randomized_svd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# conventions, so reverse its outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36msvds\u001b[1;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver)\u001b[0m\n\u001b[0;32m   1867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1868\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'arpack'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1869\u001b[1;33m         eigvals, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,\n\u001b[0m\u001b[0;32m   1870\u001b[0m                                 ncv=ncv, which=which, v0=v0)\n\u001b[0;32m   1871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[1;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unrecognized mode '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m     params = _SymmetricArpackParams(n, k, A.dtype.char, matvec, mode,\n\u001b[0m\u001b[0;32m   1685\u001b[0m                                     \u001b[0mM_matvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinv_matvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m                                     ncv, v0, maxiter, which, tol)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, k, tp, matvec, mode, M_matvec, Minv_matvec, sigma, ncv, v0, maxiter, which, tol)\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"k must be less than ndim(A), k=%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         _ArpackParams.__init__(self, n, k, tp, mode, sigma,\n\u001b[0m\u001b[0;32m    513\u001b[0m                                ncv, v0, maxiter, which, tol)\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, k, tp, mode, sigma, ncv, v0, maxiter, which, tol)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'fdFD'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrix type must be 'f', 'd', 'F', or 'D'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mv0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matrix type must be 'f', 'd', 'F', or 'D'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "# Import all of the scikit learn stuff\n",
    "from __future__ import print_function\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "module=\"pandas\", lineno=570)\n",
    "\n",
    "example = [\"Machine learning is super fun\",\n",
    "\"Python is super, super cool\",\n",
    "\"Statistics is cool, too\",\n",
    "\"Data science is fun\",\n",
    "\"Python is great for machine learning\",\n",
    "\"I like football\",\n",
    "\"Football is great to watch\"]\n",
    "vectorize = CountVectorizer(min_df=1, stop_words= 'english')\n",
    "dtm = vectorize.fit_transform(example)\n",
    "pd.DataFrame(dtm.toarray(), index=example, columns=vectorize.get_feature_names()).head(10)\n",
    "\n",
    "vectorize.get_feature_names()\n",
    "\n",
    "# Fit LSA. Use algorithm = “randomized” for large datasets\n",
    "lsa = TruncatedSVD(2, algorithm = 'arpack')\n",
    "\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "\n",
    "pd.DataFrame(lsa.components_, index = [\"component_1\", \"component_2\"], columns = vectorize.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c361df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(f'{2 * 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bc539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement keras.SGD (from versions: none)\n",
      "ERROR: No matching distribution found for keras.SGD\n"
     ]
    }
   ],
   "source": [
    "%pip install keras.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7020bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SGD (from versions: none)\n",
      "ERROR: No matching distribution found for SGD\n"
     ]
    }
   ],
   "source": [
    "%pip install SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcad7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz (778.7 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy-3.0.7-cp39-cp39-win_amd64.whl (11.5 MB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.27.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (60.1.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (0.7.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (4.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en_core_web_lg==3.0.0) (2.0.1)\n",
      "Building wheels for collected packages: en-core-web-lg\n",
      "  Building wheel for en-core-web-lg (setup.py): started\n",
      "  Building wheel for en-core-web-lg (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-lg: filename=en_core_web_lg-3.0.0-py3-none-any.whl size=778750488 sha256=d60290be458ea5586d42e4e9c969069ba3c70c2a5ecc83a065d7e3bad3151b89\n",
      "  Stored in directory: C:\\Users\\zhana\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-3ianyweo\\wheels\\d9\\e6\\b4\\310bb294577661d44ccc37971672728f968bc702706a022360\n",
      "Successfully built en-core-web-lg\n",
      "Installing collected packages: spacy, en-core-web-lg\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.1\n",
      "    Can't uninstall 'spacy'. No files were found to uninstall.\n",
      "  Attempting uninstall: en-core-web-lg\n",
      "    Found existing installation: en-core-web-lg 3.2.0\n",
      "    Uninstalling en-core-web-lg-3.2.0:\n",
      "      Successfully uninstalled en-core-web-lg-3.2.0\n",
      "Successfully installed en-core-web-lg-3.0.0 spacy-3.0.7\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 19:24:13.389746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-01-16 19:24:13.389768: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: Error parsing requirements for spacy: [Errno 2] No such file or directory: 'c:\\\\users\\\\zhana\\\\anaconda3\\\\envs\\\\studying_project\\\\lib\\\\site-packages\\\\spacy-3.2.1.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for en-core-web-sm: [Errno 2] No such file or directory: 'c:\\\\users\\\\zhana\\\\anaconda3\\\\envs\\\\studying_project\\\\lib\\\\site-packages\\\\en_core_web_sm-3.2.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for en-core-web-md: [Errno 2] No such file or directory: 'c:\\\\users\\\\zhana\\\\anaconda3\\\\envs\\\\studying_project\\\\lib\\\\site-packages\\\\en_core_web_md-3.2.0.dist-info\\\\METADATA'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.1.3 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible.\n",
      "en-vectors-web-lg 2.1.0 requires spacy<3.0.0,>=2.1.0, but you have spacy 3.0.7 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabca140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
