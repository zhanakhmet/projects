{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33edd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo rose\n",
      "Hi , How are you?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "r = \"(hi|hello|hey)[ ]*([a-z])*\"\n",
    "re.match(r, 'Hello Rosa', flags=re.IGNORECASE)\n",
    "re.match(r, \"hi ho, hi ho, its off to work...\", flags=re.IGNORECASE)\n",
    "re.match(r, \"hey, whats up\", flags=re.IGNORECASE)\n",
    "\n",
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|good[ ]?(morn[gin']{0,3}|\"\\\n",
    "    r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "re_greeting = re.compile(r, flags=re.IGNORECASE)\n",
    "re_greeting.match('Hello Rosa')\n",
    "re_greeting.match('Good morning Rosa')\n",
    "re_greeting.match('Good Manning Rosa')\n",
    "re_greeting.match('Good evening Rosa Parks').groups()\n",
    "re_greeting.match(\"Good Morn'n Rosa\")\n",
    "re_greeting.match('yo Rosa')\n",
    "\n",
    "my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 'chatterbot'])\n",
    "curt_names = set(['hal', 'you', 'u'])\n",
    "greeter_name = ''\n",
    "match = re_greeting.match(input())\n",
    "\n",
    "if match:\n",
    "    at_name = match.groups()[-1]\n",
    "    if at_name in curt_names:\n",
    "        print('Good one')\n",
    "    elif at_name.lower() in my_names:\n",
    "        print('Hi {}, How are you?'.format(greeter_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb83b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'house', 'is', 'huge']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'My house is huge'\n",
    "str.split(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a830748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab)\n",
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "', '.join(vocab)\n",
    "onehot_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0f83b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sentence = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a48008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " ',',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "sentence = \"Dr. Monticello wasn't designated as UNESCO World , Heritage Site until 1987.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88454081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)\"\"\"\n",
    "casual_tokenize(message)\n",
    "casual_tokenize(message, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de7ef2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "878207c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas, Jefferson',\n",
       " 'Jefferson, began',\n",
       " 'began, building',\n",
       " 'building, Monticello',\n",
       " 'Monticello, at',\n",
       " 'at, the',\n",
       " 'the, age',\n",
       " 'age, of',\n",
       " 'of, 26']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2))\n",
    "two_grams = list(ngrams(tokens, 2))\n",
    "[', '.join(x) for x in two_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5765425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['house', 'Visitor', 'CenTer']\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1958d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = 'The faster Harry got to the store, the faster Harry, the faster, would get home.'\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer = tokenizer.tokenize(sentence)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0625594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4),\n",
       " ('faster', 3),\n",
       " (',', 3),\n",
       " ('harry', 2),\n",
       " ('got', 1),\n",
       " ('to', 1),\n",
       " ('store', 1),\n",
       " ('would', 1),\n",
       " ('get', 1),\n",
       " ('home', 1),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = 'The faster Harry got to the store, the faster Harry, the faster, would get home.'\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "from collections import Counter\n",
    "words = Counter(tokens)\n",
    "words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f81cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = 'The faster Harry got to the store, the faster Harry, the faster, would get home.'\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "from collections import Counter\n",
    "words = Counter(tokens)\n",
    "words.most_common(4)\n",
    "times_harry_appears = words['harry']\n",
    "num_unique_words = len(words)\n",
    "tf = times_harry_appears / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e827381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06422018348623854,\n",
       " 0.06422018348623854,\n",
       " 0.03669724770642202,\n",
       " 0.022935779816513763,\n",
       " 0.01834862385321101,\n",
       " 0.01834862385321101,\n",
       " 0.013761467889908258,\n",
       " 0.013761467889908258,\n",
       " 0.013761467889908258,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that \n",
    "react against the air to create lift and drag. A kite consists of wings, tethers, \n",
    "and anchors. Kites often have a bridle to guide the face of the kite at the correct \n",
    "angle so the wind can lift it. A kite’s wing also may be so designed so a bridle is \n",
    "not needed; when kiting a sailplane for launch, the tether meets the wing at a \n",
    "single point. A kite may have fixed or moving anchors. Untraditionally in technical \n",
    "kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, \n",
    "though, a wing in the system is still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the \n",
    "kite’s surface, producing low pressure above and high pressure below the wings. \n",
    "The interaction with the wind also generates horizontal drag along the direction \n",
    "of the wind. The resultant force vector from the lift and drag force components is \n",
    "opposed by the tension of one or more of the lines or tethers to which the kite is \n",
    "attached. The anchor point of the kite line may be static or moving (such as the \n",
    "towing of a kite by a running person, boat, free-falling anchors as in paragliders \n",
    "and fugitive parakites or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under \n",
    "water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite \n",
    "lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown individually \n",
    "and at festivals worldwide. Kites may be flown for recreation, art or other practical \n",
    "uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. \n",
    "Power kites are multi-line steerable kites designed to generate large forces \n",
    "which can be used to power activities such as kite surfing, kite landboarding, \n",
    "kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites \n",
    "have been made.\"\"\"\n",
    "tokens = tokenizer.tokenize(text.lower())\n",
    "stopwords = stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value/doc_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13f363ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "len(doc_tokens[0])\n",
    "\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)\n",
    "\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "len(lexicon)\n",
    "lexicon \n",
    "\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector\n",
    "\n",
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c2c93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()[:10]\n",
    "\n",
    "brown.tagged_words()[:5]\n",
    "\n",
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c0d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030508474576271188"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that \n",
    "react against the air to create lift and drag. A kite consists of wings, tethers, \n",
    "and anchors. Kites often have a bridle to guide the face of the kite at the correct \n",
    "angle so the wind can lift it. A kite’s wing also may be so designed so a bridle is \n",
    "not needed; when kiting a sailplane for launch, the tether meets the wing at a \n",
    "single point. A kite may have fixed or moving anchors. Untraditionally in technical \n",
    "kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, \n",
    "though, a wing in the system is still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the \n",
    "kite’s surface, producing low pressure above and high pressure below the wings. \n",
    "The interaction with the wind also generates horizontal drag along the direction \n",
    "of the wind. The resultant force vector from the lift and drag force components is \n",
    "opposed by the tension of one or more of the lines or tethers to which the kite is \n",
    "attached. The anchor point of the kite line may be static or moving (such as the \n",
    "towing of a kite by a running person, boat, free-falling anchors as in paragliders \n",
    "and fugitive parakites or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under \n",
    "water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite \n",
    "lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown individually \n",
    "and at festivals worldwide. Kites may be flown for recreation, art or other practical \n",
    "uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. \n",
    "Power kites are multi-line steerable kites designed to generate large forces \n",
    "which can be used to power activities such as kite surfing, kite landboarding, \n",
    "kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites \n",
    "have been made.\"\"\"\n",
    "history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily \n",
    "available: silk fabric for sail material; fine, high-tensile-strength silk for flying \n",
    "line; and resilient bamboo for a strong, lightweight framework.\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese \n",
    "philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD paper \n",
    "kites were certainly being flown, as it was recorded that in that year a paper kite \n",
    "was used as a message for a rescue mission. Ancient and medieval Chinese sources \n",
    "describe kites being used for measuring distances, testing the wind, lifting men, \n",
    "signaling, and communication for military operations. The earliest known Chinese \n",
    "kites were flat (not bowed) and often rectangular. Later, tailless kites incorporated \n",
    "a stabilizing bowline. Kites were decorated with mythological motifs and legendary \n",
    "figures; some were fitted with strings and whistles to make musical sounds while \n",
    "flying. From China, kites were introduced to Cambodia, Thailand, India, Japan, \n",
    "Korea and the western world.\n",
    "After its introduction into India, the kite further evolved into the fighter kite, \n",
    "known as the patang in India, where thousands are flown every year on festivals \n",
    "such as Makar Sankranti.\n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the assumption \n",
    "being that the knowledge diffused from China along with the people.\n",
    "Anthropomorphic kites made from cloth and wood were used in religious \n",
    "ceremonies to send prayers to the gods. Polynesian kite traditions are used by \n",
    "anthropologists get an idea of early “primitive” Asian traditions that are believed \n",
    "to have at one time existed in Asia.\"\"\"\n",
    "kite_intro = text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "history = history.lower()\n",
    "history_tokens = tokenizer.tokenize(history)\n",
    "intro_total = len(intro_tokens)\n",
    "intro_total\n",
    "history_total = len(history_tokens)\n",
    "history_total\n",
    "\n",
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "'Term Frequency of \"kite\" in intro is: {:4f}'.format(intro_tf['kite'])\n",
    "'Term Frequency of \"kite\" in history is: {:4f}'.format(history_tf['kite'])\n",
    "\n",
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "'Term Frequency of \"and\" in intro is: {:4f}'.format(intro_tf['and'])\n",
    "'Term Frequency of \"and\" in history is: {:4f}'.format(history_tf['and'])\n",
    "\n",
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "\n",
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "        \n",
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "\n",
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total\n",
    "\n",
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china\n",
    "history_idf['china'] = num_docs / num_docs_containing_china\n",
    "\n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']\n",
    "\n",
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']\n",
    "\n",
    "history_tfidf['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0c7a977",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_496/1216587094.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\zhana\\AppData\\Local\\Temp/ipykernel_496/1216587094.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    topic['petness'] = (.3 * tfidf['cat'] +\\\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "topic = {}\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6)))\n",
    "topic['petness'] = (.3 * tfidf['cat'] +\\\n",
    "                    .3 * tfidf['dog'] +\\\n",
    "                     0 * tfidf['apple'] +\\\n",
    "                     0 * tfidf['lion'] -\\\n",
    "                    .2 * tfidf['NYC'] +\\\n",
    "                    .2 * tfidf['love'])\n",
    "topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "                       .1 * tfidf['dog'] -\\\n",
    "                       .1 * tfidf['apple'] +\\\n",
    "                       .5 * tfidf['lion'] +\\\n",
    "                       .1 * tfidf['NYC'] +\\\n",
    "                       .1 * tfidf['love'])\n",
    "topic['cityness'] = (0 * tfidf['cat'] +\\\n",
    "                    .1 * tfidf['dog'] +\\\n",
    "                    .2 * tfidf['apple'] -\\\n",
    "                    .1 * tfidf['lion'] +\\\n",
    "                    .5 * tfidf['NYC'] +\\\n",
    "                    .1 * tfidf['love'])\n",
    "word_topic_vectors = [\"NYC is the Big Apple.\"]\n",
    "word_topic_vectors.append(\"NYC is known as the Big Apple.\")\n",
    "word_topic_vectors.append(\"I love NYC!\")\n",
    "word_topic_vectors.append(\"I wore a hat to the Big Apple party in NYC.\")\n",
    "word_topic_vectors.append(\"Come to NYC. See the Big Apple!\")\n",
    "word_topic_vectors.append(\"Mahhattan is called the Big Apple.\")\n",
    "word_topic_vectors.append(\"New York is a big city for a small cat.\")\n",
    "word_topic_vectors.append(\"The lion, a big cat, is the king of the jungle.\")\n",
    "word_topic_vectors.append(\"I love my pet cat.\")\n",
    "word_topic_vectors.append(\"I love New York City (NYC).\")\n",
    "word_topic_vectors.append(\"Your dog chassed my cat.\")\n",
    "bow_svd, tfidf_svd = lsa_models()\n",
    "prettify_tdm(**bow_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82c15b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>was</th>\n",
       "      <th>done</th>\n",
       "      <th>mostly</th>\n",
       "      <th>by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thomas  Jefferson  began  building  ...  was  done  mostly  by\n",
       "sent0       1          1      1         1  ...    0     0       0   0\n",
       "sent1       0          0      0         0  ...    1     1       1   1\n",
       "sent2       0          0      0         0  ...    0     0       0   0\n",
       "sent3       0          0      0         0  ...    1     0       0   0\n",
       "\n",
       "[4 rows x 15 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab)\n",
    "num_tokens = len(token_sequence)\n",
    "vocab_num = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_num), int)\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "    ', '.join(vocab)\n",
    "onehot_vectors\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "\n",
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()])), columns=['sent']).T\n",
    "df\n",
    "\n",
    "sentence += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentence += \"\"\"He moved into the South Pavilion in 1770.\\n\"\"\"\n",
    "sentence += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df[df.columns[:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2917c7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "len(doc_tokens[0])\n",
    "\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)\n",
    "\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "len(lexicon)\n",
    "lexicon \n",
    "\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector\n",
    "\n",
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors\n",
    "\n",
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for doc in docs:\n",
    "            if key in doc:\n",
    "                docs_containing_key += 1\n",
    "            tf = value / len(lexicon)\n",
    "            if docs_containing_key:\n",
    "                idf = len(docs) / docs_containing_key\n",
    "            else:\n",
    "                idf = 0\n",
    "            vec[key] = tf * idf\n",
    "        document_tfidf_vectors.append(vec)\n",
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0610645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "example_input = [1, .2, .1, .05, .2]\n",
    "example_weight = [.2, .12, .4, .6, .90]\n",
    "\n",
    "input_vector = np.array(example_input)\n",
    "weights = np.array(example_weight)\n",
    "bias_weight = .2\n",
    "\n",
    "activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
    "activation_level\n",
    "\n",
    "threshold = 0.5\n",
    "if activation_level > threshold:\n",
    "    perceptron_output = 1\n",
    "else:\n",
    "    perceptron_output = 0\n",
    "perceptron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c04a8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 0\n",
      "Expected: 0\n",
      "\n",
      "Predicted 0\n",
      "Expected: 1\n",
      "\n",
      "Predicted 0\n",
      "Expected: 1\n",
      "\n",
      "Predicted 0\n",
      "Expected: 1\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18448/4087303733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mnew_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mnew_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexpected_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mperceptron_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mbias_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbias_weight\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mperceptron_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "sample_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "expected_results = [0, 1, 1, 1]\n",
    "activation_threshold = 0.5\n",
    "\n",
    "weights = np.random.random(2)/1000 # Small random float 0 < w < .001\n",
    "weights\n",
    "\n",
    "bias_weight = np.random.random() / 1000  #Смещение\n",
    "bias_weight\n",
    "\n",
    "for idx, sample in enumerate(sample_data):\n",
    "    input_vector = np.array(sample)\n",
    "    activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
    "    if activation_level > activation_threshold:\n",
    "        perceptron_output = 1\n",
    "    else:\n",
    "        perceptron_output = 0\n",
    "    print('Predicted {}'.format(perceptron_output))\n",
    "    print('Expected: {}'.format(expected_results[idx]))\n",
    "    print()\n",
    "\n",
    "for iteration_num in range(5):\n",
    "    correct_answer = 0\n",
    "    for idx, sample in enumerate(sample_data):\n",
    "        input_vector = np.array(sample)\n",
    "        weights = np.array(weights)\n",
    "        activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
    "        if activation_level > activation_threshold:\n",
    "            perceptron_output = 1\n",
    "        else:\n",
    "            perceptron_output = 0\n",
    "        if perceptron_output == expected_results[idx]:\n",
    "            correct_answer += 1\n",
    "        new_weight = []\n",
    "        for i, x in enumerate(sample):\n",
    "            new_weight.append(weights[i] + (expected_results[idx] - perceptron_output) * x)\n",
    "            bias_weight = bias_weight + ((expected_results[idx] - perceptron_output) * 1)\n",
    "            weights = np.array(new_weight)\n",
    "        print('{} correct answer out of 4, for iteration {}'.format(correct_answer, iteration_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbbacb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.17149858514250882\n",
      "  (0, 7)\t0.17149858514250882\n",
      "  (0, 4)\t0.17149858514250882\n",
      "  (0, 6)\t0.17149858514250882\n",
      "  (0, 2)\t0.17149858514250882\n",
      "  (0, 3)\t0.34299717028501764\n",
      "  (0, 0)\t0.5144957554275265\n",
      "  (0, 5)\t0.6859943405700353\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13844\\3167654460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2757\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1634\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1635\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2281\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'_child{len(self._children)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2304\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \"\"\"\n\u001b[1;32m-> 2306\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2308\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;34m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sentence = ['The faster Harry got to the store, the faster Harry, the faster, would get']\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(sentence)\n",
    "print(x)\n",
    "\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f33bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
