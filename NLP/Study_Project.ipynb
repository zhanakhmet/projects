{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dc9b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print('Hi') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc8c31",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} & = \\frac{4\\pi}{c}\\vec{\\mathbf{j}} \\\\   \\nabla \\cdot \\vec{\\mathbf{E}} & = 4 \\pi \\rho \\\\\n",
    "\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t} & = \\vec{\\mathbf{0}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{B}} & = 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70153d91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a30ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Out[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e29ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069c4f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8452/3593848235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7d67ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\zhana'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f37efb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "123\n",
      "123\n",
      "123\n",
      "123\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5):\n",
    "    print('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e7ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n",
      "12345\n",
      "12345\n",
      "12345\n",
      "12345\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5):\n",
    "    print('12345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d941876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22492201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "x = int(input(''))\n",
    "y = int(input(''))\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab489b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " 'will',\n",
       " 'help',\n",
       " 'you',\n",
       " 'add',\n",
       " 'an',\n",
       " 'extra',\n",
       " 'skill',\n",
       " 'and',\n",
       " 'also',\n",
       " 'enhance',\n",
       " 'your',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Learning',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " 'is',\n",
       " 'also',\n",
       " 'beneficial',\n",
       " 'for',\n",
       " 'professionals',\n",
       " 'to',\n",
       " 'enhance',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'in',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'with',\n",
       " 'Python',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = 'Learning Natural Language Toolkit will help you add an extra skill and also enhance your knowledge of NLP. Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.'\n",
    "a = word_tokenize(sent)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e18a9db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning Natural Language Toolkit will help you add an extra skill and also enhance your knowledge of NLP.',\n",
       " 'Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = 'Learning Natural Language Toolkit will help you add an extra skill and also enhance your knowledge of NLP. Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.'\n",
    "a = word_tokenize(sent)\n",
    "b = sent_tokenize(sent)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "707bd374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " 'help',\n",
       " 'add',\n",
       " 'extra',\n",
       " 'skill',\n",
       " 'also',\n",
       " 'enhance',\n",
       " 'knowledge',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Learning',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " 'also',\n",
       " 'beneficial',\n",
       " 'professionals',\n",
       " 'enhance',\n",
       " 'careers',\n",
       " 'AI',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'Python',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "sent = 'Learning Natural Language Toolkit will help you add an extra skill and also enhance your knowledge of NLP. Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.'\n",
    "a = word_tokenize(sent)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(word) for word in sent]\n",
    "\n",
    "cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cb285fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'toolkit',\n",
       " 'help',\n",
       " 'add',\n",
       " 'extra',\n",
       " 'skill',\n",
       " 'also',\n",
       " 'enhanc',\n",
       " 'knowledg',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'learn',\n",
       " 'nltk',\n",
       " 'librari',\n",
       " 'also',\n",
       " 'benefici',\n",
       " 'profession',\n",
       " 'enhanc',\n",
       " 'career',\n",
       " 'ai',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'python',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "sent = 'Learning Natural Language Toolkit will help you add an extra skill and also enhance your knowledge of NLP. Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.'\n",
    "a = word_tokenize(sent)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(word) for word in cleaned_token]\n",
    "\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82cfd2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learn', 'NN'),\n",
       " ('natur', 'DT'),\n",
       " ('languag', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('toolkit', 'NN'),\n",
       " ('help', 'NN'),\n",
       " ('add', 'VB'),\n",
       " ('extra', 'JJ'),\n",
       " ('skill', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('enhanc', 'VBZ'),\n",
       " ('knowledg', 'NN'),\n",
       " ('nlp', 'NN'),\n",
       " ('.', '.'),\n",
       " ('learn', 'VB'),\n",
       " ('nltk', 'JJ'),\n",
       " ('librari', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('benefici', 'JJ'),\n",
       " ('profession', 'NN'),\n",
       " ('enhanc', 'NN'),\n",
       " ('career', 'NN'),\n",
       " ('ai', 'VBP'),\n",
       " ('natur', 'JJ'),\n",
       " ('languag', 'NN'),\n",
       " ('process', 'NN'),\n",
       " ('python', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import pos_tag\n",
    "\n",
    "sent = \"Learning Natural Language's Toolkit will help you add an extra skill and also enhance your knowledge of NLP. Learning NLTK library is also beneficial for professionals to enhance their careers in AI and Natural Language Processing with Python.\"\n",
    "a = word_tokenize(sent)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(word) for word in cleaned_token]\n",
    "\n",
    "tagged = pos_tag(stemmed)\n",
    "\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03872e10",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zhana/nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zhana/nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12620/2864362619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"better\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1174\u001b[0m             )\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Studying_Project\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zhana/nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\anaconda3\\\\envs\\\\Studying_Project\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zhana\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ac03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement spacy-model-en_core_web_trf-3.2.0 (from versions: none)\n",
      "ERROR: No matching distribution found for spacy-model-en_core_web_trf-3.2.0\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy-model-en_core_web_trf-3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7672cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-nightly\n",
      "  Downloading spacy_nightly-3.0.0rc5-cp39-cp39-win_amd64.whl (11.4 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (2.27.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (0.7.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (2.4.2)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.4-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "Requirement already satisfied: pathy in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (0.9.0)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (3.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (60.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (1.21.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (8.0.13)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0.dev0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from spacy-nightly) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from packaging>=20.0->spacy-nightly) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy-nightly) (0.4.4)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from jinja2->spacy-nightly) (2.0.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\zhana\\anaconda3\\envs\\studying_project\\lib\\site-packages (from pathy->spacy-nightly) (5.2.1)\n",
      "Installing collected packages: click, typer, pydantic, spacy-nightly\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.3\n",
      "    Uninstalling click-8.0.3:\n",
      "      Successfully uninstalled click-8.0.3\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.0\n",
      "    Uninstalling typer-0.4.0:\n",
      "      Successfully uninstalled typer-0.4.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.8.2\n",
      "    Uninstalling pydantic-1.8.2:\n",
      "      Successfully uninstalled pydantic-1.8.2\n",
      "Successfully installed click-7.1.2 pydantic-1.7.4 spacy-nightly-3.0.0rc5 typer-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-vectors-web-lg 2.1.0 requires spacy<3.0.0,>=2.1.0, but you have spacy 3.2.1 which is incompatible.\n",
      "UsageError: Line magic function `%%python` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy-nightly --pre\n",
    "%%python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
