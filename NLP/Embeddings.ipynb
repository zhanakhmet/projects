{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60d7061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jasons']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['jasons', 'ted']\n",
    "word = []\n",
    "[word for word in words if len(word) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcf018f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "better.\n",
      "Text\n",
      "preprocessing\n",
      "includes\n",
      "both\n",
      "Stemming\n",
      "a\n",
      "well\n",
      "a\n",
      "Lemmatization.\n",
      "Many\n",
      "time\n",
      "people\n",
      "find\n",
      "these\n",
      "two\n",
      "term\n",
      "confusing.\n",
      "Some\n",
      "treat\n",
      "these\n",
      "two\n",
      "a\n",
      "the\n",
      "same.\n",
      "Actually\n",
      ",\n",
      "lemmatization\n",
      "is\n",
      "preferred\n",
      "over\n",
      "Stemming\n",
      "because\n",
      "lemmatization\n",
      "doe\n",
      "morphological\n",
      "analysis\n",
      "of\n",
      "the\n",
      "word\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kitty', 'better']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import TreebankWordTokenizer\n",
    "\n",
    "text = \"I am better. Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as the same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokened_text = tokenizer.tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in tokened_text:\n",
    "    print(lemmatizer.lemmatize(word))\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdd4d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitty', 'better']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import TreebankWordTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "text = ['kitty', 'better']\n",
    "lemmatized_text = [lemmatizer.lemmatize(words) for words in text]\n",
    "lemmatized_text\n",
    "\n",
    "stemmed_text = [stemmer.stem(words) for words in text]\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7468e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c654e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')  \n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb31c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Let's go!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "text = \"Let's go!\"\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5444c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'be', 'read', 'the', 'paper', '!', 'run', 'testing', 'well']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "doc = nlp('I was reading the paper! running testing better')\n",
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e996eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "was\n",
      "reading\n",
      "the\n",
      "paper\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "doc = nlp('I was reading the paper!')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c10e8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This True DT DET\n",
      "is True VBZ AUX\n",
      "just True RB ADV\n",
      "a True DT DET\n",
      "sample False NN NOUN\n",
      "text False NN NOUN\n",
      "for True IN ADP\n",
      "the True DT DET\n",
      "purpose False NN NOUN\n",
      "of True IN ADP\n",
      "testing False VBG VERB\n",
      "anti False JJ ADJ\n",
      "- False JJ ADJ\n",
      "buff False JJ ADJ\n",
      "better False JJR ADJ\n",
      "organizing False NN NOUN\n",
      "running False VBG VERB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[sample, text, purpose, organizing]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy #Import Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #Initialize the Spacy en mode\n",
    "My_text = \"This is just a sample text for the purpose of testing anti-buff better organizing running\"\n",
    "doc = nlp(My_text) #Parse the text\n",
    "stop = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "tokens = [token.lemma_ for token in stop]\n",
    "tokens\n",
    "for token in doc:\n",
    "    print(token, token.is_stop, token.tag_, token.pos_)\n",
    "\n",
    "nouns = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07b7b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus False\n",
      "Proto False\n",
      "is True\n",
      "a True\n",
      "Python False\n",
      "developer False\n",
      "currently False\n",
      "working False\n",
      "for True\n",
      "a True\n",
      "London False\n",
      "- False\n",
      "based False\n",
      "Fintech False\n",
      "company False\n",
      ". False\n",
      "He True\n",
      "is True\n",
      "interested False\n",
      "in True\n",
      "learning False\n",
      "Natural False\n",
      "Language False\n",
      "Processing False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_text = ('Gus Proto is a Python developer currently'\n",
    "               ' working for a London-based Fintech'\n",
    "               ' company. He is interested in learning'\n",
    "               ' Natural Language Processing.')\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for token in about_doc:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75505e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
